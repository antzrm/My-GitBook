# Race Conditions

A common vuln in web apps where concurrent processes lead to unintended behavior.

## Limit overrun race conditions

For example, consider an online store that lets you enter a promotional code during checkout to get a one-time discount on your order. To apply this discount, the application may perform the following high-level steps:

1\. Check that you haven't already used this code.

2\. Apply the discount to the order total.

3\. Update the record in the database to reflect the fact that you've now used this code.

If you later attempt to reuse this code, the initial checks performed at the start of the process should prevent you from doing this:

<figure><img src="../../.gitbook/assets/image (145).png" alt=""><figcaption></figcaption></figure>

Now consider what would happen if a user who has never applied this discount code before tried to apply it twice at almost exactly the same time:

<figure><img src="../../.gitbook/assets/image (146).png" alt=""><figcaption></figcaption></figure>

As you can see, the application transitions through a temporary sub-state; that is, a state that it enters and then exits again before request processing is complete. In this case, the sub-state begins when the server starts processing the first request, and ends when it updates the database to indicate that you've already used this code. This introduces a small race window during which you can repeatedly claim the discount as many times as you like.

There are many variations of this kind of attack, including:

• Redeeming a gift card multiple times

• Rating a product multiple times

• Withdrawing or transferring cash in excess of your account balance

• Reusing a single CAPTCHA solution

• Bypassing an anti-brute-force rate limit

Limit overruns are a subtype of so-called "time-of-check to time-of-use" (TOCTOU) flaws. Later in this topic, we'll look at some examples of race condition vulnerabilities that don't fall into either of these categories.

## Detecting and exploiting limit overrun race conditions with Burp Repeater

1\. Identify a single-use or rate-limited endpoint that has some kind of security impact or other useful purpose.

2\. Issue multiple requests to this endpoint in quick succession to see if you can overrun this limit.

The primary challenge is timing the requests so that at least two race windows line up, causing a collision. This window is often just milliseconds and can be even shorter.

Even if you send all of the requests at exactly the same time, in practice there are various uncontrollable and unpredictable external factors that affect when the server processes each request and in which order.

<figure><img src="../../.gitbook/assets/image (147).png" alt=""><figcaption></figcaption></figure>

Burp Suite 2023.9 adds powerful new capabilities to Burp Repeater that enable you to easily send a group of parallel requests in a way that greatly reduces the impact of one of these factors, namely network jitter. Burp automatically adjusts the technique it uses to suit the HTTP version supported by the server:

• For HTTP/1, it uses the classic last-byte synchronization technique.

• For HTTP/2, it uses the single-packet attack technique, first demonstrated by PortSwigger Research at Black Hat USA 2023.

The single-packet attack enables you to completely neutralize interference from network jitter by using a single TCP packet to complete 20-30 requests simultaneously.

<figure><img src="../../.gitbook/assets/image (148).png" alt=""><figcaption></figcaption></figure>

```sh
# EXAMPLE: apply a coupon code multiple times
- Send request applying coupon code to Repeater
- Clone/copy that request to have like 20 on Repeater
- Move tab to group -> Create tab group -> Place all requests under the same group
- Click on Send > Send group in parallel
```

## Detecting and exploiting limit overrun race conditions with Turbo Intruder

Turbo Intruder extension also supports this technique.

Turbo Intruder requires some proficiency in Python, but is suited to more complex attacks, such as ones that require multiple retries, staggered request timing, or an extremely large number of requests.

{% code overflow="wrap" fullWidth="true" %}
```sh
 To use the single-packet attack in Turbo Intruder:
    Ensure that the target supports HTTP/2. The single-packet attack is incompatible with HTTP/1.
    Set the engine=Engine.BURP2 and concurrentConnections=1 configuration options for the request engine.
    When queueing your requests, group them by assigning them to a named gate -> gate argument for the engine.queue() method.
    To send all of the requests in a given group, open the respective gate with the engine.openGate() method.
def queueRequests(target, wordlists):
    engine = RequestEngine(endpoint=target.endpoint,
                            concurrentConnections=1,
                            engine=Engine.BURP2
                            )
    
    # queue 20 requests in gate '1'
    for i in range(20):
        engine.queue(target.req, gate='1')
    
    # send all requests in gate '1' in parallel
    engine.openGate('1')
For more details, see the race-single-packet-attack.py template provided in Turbo Intruder default examples directory.         

# EXAMPLE: Bypassing rate limits via race conditions (having a target user and a bunch of passwords to bf login)
- Try incorrect logins. When we fail, we see "Invalid user or password". After 3+ attempts, locked account.
- Send login request to Repeater > Get 20 of them > Group
- Send requests in parallel and find all of those show "Invalid user or password" instead of locked account.
- The previous result meant if we try different passwords quickly enough, we can make it with no account lock.
- From login request, select password and then Extensions > Turbo Intruder
- Change username to our target user. Note password value became %s and select single-packet-attack.py script
def queueRequests(target, wordlists):
    engine = RequestEngine(endpoint=target.endpoint,
                            concurrentConnections=1,
                            engine=Engine.BURP2
                            )
    

    for word in wordlists.clipboard: # loop through all passwords (we copy them to our clipboard first)
        engine.queue(target.req, word, gate='1')
    
    # send all requests in gate '1' in parallel
    engine.openGate('1')
...
- Run the attack and check status code to find a different one.
# RESOURCES
https://www.intigriti.com/researchers/blog/hacking-tools/hacker-tools-turbo-intruder
https://portswigger.net/research/turbo-intruder-embracing-the-billion-request-attack
```
{% endcode %}

## Hidden multi-step sequences

In practice, a single request may initiate an entire multi-step sequence behind the scenes, transitioning the application through multiple hidden states that it enters and then exits again before request processing is complete. We'll refer to these as "sub-states".

If you can identify one or more HTTP requests that cause an interaction with the same data, you can potentially abuse these sub-states to expose time-sensitive variations of the kinds of logic flaws that are common in multi-step workflows. This enables race condition exploits that go far beyond limit overruns.

For example, you may be familiar with flawed multi-factor authentication (MFA) workflows that let you perform the first part of the login using known credentials, then navigate straight to the application via forced browsing, effectively bypassing MFA entirely.

{% code overflow="wrap" fullWidth="true" %}
```sh
#  The following pseudo-code demonstrates how a website could be vulnerable to a race variation of this attack:

    session['userid'] = user.userid
    if user.mfa_enabled:
    session['enforce_mfa'] = True
    # generate and send MFA code to user
    # redirect browser to MFA code entry form
    
# it transitions through a sub-state in which the user temporarily has a valid session, but MFA isn't yet being enforced
# An attacker could exploit this by sending a login request along with a request to a sensitive, authenticated endpoint.     
```
{% endcode %}

## Methodology for identifying and exploiting race conditions

<figure><img src="../../.gitbook/assets/image (149).png" alt=""><figcaption></figcaption></figure>

{% code overflow="wrap" fullWidth="true" %}
```sh
########################### 1 - Predict potential collisions
After mapping out the target site as normal, you can reduce the number of endpoints that you need to test by asking:
- iS this endpoint security critical? Many endpoints do not touch critical functionality, so they are not worth testing.
- Is there any collision potential? For a successful collision, you typically need two or more requests 
that trigger operations on the same record. 
For example, consider the following variations of a password reset implementation:
```
{% endcode %}

<figure><img src="../../.gitbook/assets/image (150).png" alt=""><figcaption></figcaption></figure>

{% code overflow="wrap" fullWidth="true" %}
```sh
With the first example, requesting parallel password resets for two different users is unlikely to cause 
a collision as it results in changes to two different records. 
However, the second implementation enables you to edit the same record with requests for two different users.

############################ 2 - Probe for clues
To recognize clues, you first need to benchmark how the endpoint behaves under normal conditions. 
Repeater > group all requests > Send group in sequence (separate connections)
Then same group of requests at once using the single-packet attack > Repeater > Send group in parallel
Anything at all can be a clue. Just look for some form of deviation from what you observed during benchmarking.

############################ 3 - Prove the concept
Try to understand what is happening, remove superfluous requests, and make sure you can still replicate the effects. 
Advanced race conditions can cause unusual and unique primitives
# Resources
https://portswigger.net/research/smashing-the-state-machine
```
{% endcode %}

## Multi-endpoint race conditions

Perhaps the most intuitive form of these race conditions are those that involve sending requests to multiple endpoints at the same time.

Think about the classic logic flaw in online stores where you add an item to your basket or cart, pay for it, then add more items to the cart before force-browsing to the order confirmation page.

A variation of this vulnerability can occur when payment validation and order confirmation are performed during the processing of a single request. The state machine for the order status might look something like this:

<figure><img src="../../.gitbook/assets/image (151).png" alt=""><figcaption></figcaption></figure>

In this case, you can potentially add more items to your basket during the race window between when the payment is validated and when the order is finally confirmed.

### Aligning multi-endpoint race windows

When testing for multi-endpoint race conditions, you may encounter issues trying to line up the race windows for each request, even if you send them all at exactly the same time using the single-packet technique.

<figure><img src="../../.gitbook/assets/image (152).png" alt=""><figcaption></figcaption></figure>

This common problem is primarily caused by the following two factors:

* **Delays introduced by network architecture** (between front-end and backend, protocol used..)
* **Delays introduced by endpoint-specific processing -** Different endpoints inherently vary in their processing times, depending on what operations they trigger.

Fortunately, there are potential workarounds to both of these issues.

### Connection warming

Back-end connection delays don't usually interfere with race condition attacks because they typically delay parallel requests equally, so the requests stay in sync.

It's essential to be able to distinguish these delays from those caused by endpoint-specific factors. One way -> "warming" the connection with one or more inconsequential requests -> Repeater > add a GET request for the homepage to the start of your tab group > Send group in sequence (single connection).

If the first request still has a longer processing time, but the rest of the requests are now processed within a short window, you can ignore the apparent delay and continue testing.

If you still see inconsistent response times on a single endpoint, even when using the single-packet technique, this is an indication that the back-end delay is interfering with your attack. You may be able to work around this by using Turbo Intruder to send some connection warming requests before following up with your main attack requests.
